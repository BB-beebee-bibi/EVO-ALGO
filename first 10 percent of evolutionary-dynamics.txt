IN 1831, at the age of twenty-two, Charles Darwin embarked on his journey around the world. He gazed at the breath-taking diversity of tropical flora and fauna, collected creepy-crawlies from the vast oceans that he traversed, was hopelessly seasick, saw slavery in Brazil, witnessed genocide in Argentina, and was underwhelmed by the naked humanity at Tierra del Fuego. He experienced the effects of a devastating earthquake in Chile that raised the South American continent. He led an expedition into the Andes and discovered marine fossils at high altitude. He paid little attention to which finches came from which islands in the Galápagos and ate most of the delicious turtles he had gathered on his way home across the Pacific. He saw Tahiti and the economic rise of Australia. He visited John Hershel, England’s leading physicist of the time, in South Africa; Hershel told him that “the mystery of mysteries” was the as yet unknown mechanism that gave rise to new species. Darwin returned to England’s shores after five years, having collected six thousand specimens that would require decades of analysis by an army of experts. His own observations in geology and the theory of his mentor, Sir Charles Lyell, that mountains were not lifted up in one day, but rose slowly over unimaginable periods of time, led Darwin to a key idea: given enough time everything can happen. Charles Darwin did not invent the concept of evolution. When he was a student in Edinburgh in the late 1820s, evolution was already the talk of the town. But evolution was rejected by the establishment. Those who adhered to evolutionary thinking were called Lamarckists, after the French scientist Jean-Baptiste Lamarck, who was the first to propose that species are not static, but change over time and give rise to new species. Lamarck had offered this perspective in a book published in 1809. He did not, however, propose a correct mechanism for how species change into each other. This mechanism was discovered first by Charles Darwin and independently by Alfred Russel Wallace. From reading the economist Thomas Malthus, Darwin was aware of the consequences of exponentially growing populations. Once resources become limiting only a fraction of individuals can survive. Darwin was also a keen observer of animal breeders. He analyzed their methods and studied their results. Slowly he understood that nature acted like a gigantic breeder. This was the first time that natural selection materialized as an idea, a scientific concept in a human mind. Darwin was thirty-three years old. The one problem that Darwin did not solve concerned the mechanism that could maintain enough diversity in a population for natural selection to operate. Darwin was unaware of the Austrian monk and botanist Gregor Mendel and his experiments on plant heredity. Mendel’s work had already been published but was hidden, gathering dust in the Annals of the Brno Academy of Sciences. Darwin once remarked, “I have deeply regretted that I did not proceed far enough at least to understand something of the great leading principles of mathematics; for men thus endowed seem to have an extra sense.” The engineer Fleeming Jenkins, who reviewed Darwin’s On the Origin of Species, published in 1859, had raised a fundamental and seemingly intractable objection to Darwin’s theory: if offspring inherit a blend of the parents’ characteristics, then variability diminishes in successive generations. Several decades later a simple mathematical equation, independently found by the famous British mathematician G. H. Hardy and the German physician Wilhelm Weinberg, showed that Mendelian (particulate) inheritance does lead to a maintainance of genetic diversity under random mating. The Hardy-Weinberg law is one of the fundamental principles of evolution under sexual reproduction. Mendelian genetics and Darwinian evolution were unified in the new discipline of mathematical biology, which developed from the seminal investigations of Ronald Fisher, J. B. S. Haldane, and Sewall Wright in the 1920s and 1930s. Through their work, fundamental concepts of evolution, selection, and mutation were embedded in a precise mathematical framework. This line of mathematical analysis was taken up in the 1950s by Motoo Kimura, who formulated the neutral theory of evolution. Kimura realized that most genetic mutations do not affect fitness and are fixed in populations only by random drift. Other milestones of evolutionary dynamics include William Hamilton’s discovery in 1964 that selection of “selfish genes” can favor altruistic behavior among relatives and John Maynard Smith’s invention of evolutionary game theory in 1973. In the mid-1970s Robert May revolutionized the mathematical approaches to ecology and epidemiology. Manfred Eigen and Peter Schuster formulated quasispecies theory, which provides a link between genetic evolution, physical chemistry, and information theory. Peter Taylor, Josef Hofbauer, and Karl Sigmund studied the replicator equation, the foundation of evolutionary game dynamics. This very brief and incomplete account of the evolution of evolutionary dynamics brings us to the present book. It has fourteen chapters. Although there is some progression of complexity, the chapters are largely independent. Therefore, if you know something about the subject, you can read the book in whatever order you like. My aim has been to keep things as simple as possible, as linear as possible, and as deterministic as possible. I will start with the basics and in a few steps lead you to some of the most interesting and unanswered research questions in the field. Having read the book, you will know what you need to embark on your own journey and make your own discoveries. This book represents an introduction to certain aspects of mathematical biology, but it is not comprehensive. Mathematical biology includes many topics, such as theoretical ecology, population genetics, epidemiology, theoretical immunology, protein folding, genetic regulatory networks, neural networks, genomic analysis, and pattern formation. The field is too diverse for any one book to represent it without running the risk of becoming as entertaining as a telephone directory. I have chosen those topics that I know well and where my explanation can be brief and effective. I have concentrated on evolution because it is the one unifying principle of all of biology. It might seem surprising that a book on evolutionary dynamics is not primarily about population genetics. Nevertheless the ideas and concepts of this fascinating field stand behind many of my explorations: the basic mathematical formulations of selection, mutation, random drift, fitness landscapes, and frequency-dependent selection as well as of evolution in structured populations have originated in population genetics. Several major themes of population genetics, however, such as sexual reproduction, sexual selection, recombination, and speciation, are not discussed here. In contrast, classical population genetics does not deal with evolutionary dynamics of infectious agents, the somatic evolution of cancer, evolutionary game theory, or the evolution of human language, all of which are subjects that I do explore. The main ingredients of evolutionary dynamics are reproduction, mutation, selection, random drift, and spatial movement. Always keep in mind that the population is the fundamental basis of any evolution. Individuals, genes, or ideas can change over time, but only populations evolve. The structure of the book is as follows. After this introduction, in Chapter 2 I will discuss populations of reproducing individuals and the basic ideas of natural selection and mutation. Simple models of population dynamics can lead to an exponential explosion, to a stable equilibrium, or to oscillations and chaos. Selection emerges whenever two or more individuals reproduce at different rates. Mutation means that one type can change into another. There are models of population growth that lead to the survival of whoever reproduces fastest (“survival of the fittest”). Other models lead to the survival of the first or the coexistence of all. In Chapter 3, quasispecies theory is introduced. Quasispecies are populations of reproducing genomes subject to mutation and selection. They live in sequence space and move over fitness landscapes. An important relationship between mutation rates and genome length is called the “error threshold”: adaptation on most fitness landscapes is possible only if the mutation rate per base is less than one over the genome length, measured in bases. In Chapter 4, we study evolutionary game dynamics, which arise whenever the fitness of an individual is not constant but depends on the relative abundance (= frequency) of others in the population. Thus evolutionary game theory is the most comprehensive way to look at the world. People who do not engage in evolutionary game theory restrict themselves to the rigidity of constant selection, where the fitness of one individual does not depend on others. The replicator equation is a nonlinear differential equation that describes frequency-dependent selection among a fixed number of strategies. We will encounter the Nash equilibrium and evolutionarily stable strategies. Evolutionary game theory and ecology are linked in an important way: the replicator equation is equivalent to the Lotka-Volterra equation of ecological systems, which describes the interation between predator and prey species. Chapter 5 is dedicated to the best game in town, the Prisoner’s Dilemma. The cooperation of reproducing entities is essential for evolutionary progress. Genes cooperate to form a genome. Cells cooperate to produce multicellular organisms. Individuals cooperate to form groups and societies. The emergence of human culture is a cooperative enterprise. The very problem of how to obtain cooperation by natural selection is described by the Prisoner’s Dilemma. In the absence of any other assumption, natural selection favors defectors over cooperators. Cooperation has a chance, however, if there are repeated interactions between the same two individuals. We will encounter the strategy Tit-for-tat, which is defeated first by Generous Tit-for-tat and then by Win-stay, lose-shift. In Chapter 6 we move to a stochastic description of finite populations. Neutral drift is a crucial aspect of evolutionary dynamics: if a finite population consists of two types of individuals, red and blue, and if both individuals have identical fitness, then eventually the population will be either all red or all blue. Even in the absence of selection, coexistence is not possible. If there is a fitness difference, then the fitter type has a greater chance of winning, but no certainty. We calculate the probability that the descendants of one individual will take over the whole population. This so-called fixation probability is important for estimating the rate of evolution. Chapter 7 is about games in finite populations. Most of evolutionary game theory has been formulated in terms of deterministic dynamics describing the limit of infinitely large populations. Here we move game theory to finite populations and make surprising observations. Neither a Nash equilibrium, nor an evolutionarily stable strategy, nor a risk-dominant strategy is protected by natural selection. There can be advantageous mutants against all three. In a bistable situation between two strategies, there is a simple “1/3 rule” that determines whether a strategy is favored by natural selection. In Chapter 8, the individuals of a population are represented by the vertices of a graph. The edges of the graph specify who interacts with whom. The graph can denote spatial relationships or social networks. The first observations of “evolutionary graph theory” are reported. The classical homogeneous population is defined by the complete graph, where all vertices are connected. We will see that circulations have the same evolutionary behavior as the complete graph in terms of fixation probability under constant selection, and therefore represent a particular balance between drift and selection. Graphs that enhance drift act as suppressors of selection. Graphs that reduce drift act as amplifiers of selection. In the limit of large population size, there exist graphs that guarantee the fixation of any advantageous mutant and the extinction of any disadvantageous mutant. Games on graphs are also studied in this chapter. There is a remarkably simple rule for the evolution of cooperation. Chapter 9 gives an account of evolutionary game dynamics on spatial grids. The primary approach will be deterministic, discrete in time, and discrete in space. This approach brings together game theory and cellular automata. We will observe evolutionary kaleidoscopes, dynamic fractals, and spatial chaos. There is all the complexity one could ever wish for—making it unnecessary for God to play dice. Moreover, cooperation can evolve on spatial grids. This is the concept of “spatial reciprocity.” In Chapter 10, we study the evolutionary dynamics of virus infections. I will argue that the mechanism of disease progression caused by the human immunodeficiency virus (HIV) is an evolutionary one. The immune system constantly attacks the virus, but the virus continuously evolves away, appears elsewhere in sequence space, and eventually overpowers the immune system. The resulting “diversity threshold theory” can explain why people succumb to the fatal immunodeficiency disease AIDS after a long and variable infection with HIV. Chapter 11 discusses the evolution of infectious agents, their attempts to infect new hosts, and the selection pressures that determine the level of virulence. The conventional wisdom is that well-adapted parasites are harmless to their hosts. This perspective is revised in the context of evolutionary dynamics. Competition between different mutants of a parasite maximizes its basic reproductive ratio. Superinfection takes into account that parasites compete on two levels of selection: within an infected host and in the population of hosts. Superinfection holds many surprising aspects, including the shortsighted evolution of higher and higher virulence beyond what would be optimum for the parasite. Chapter 12 explores the evolutionary dynamics of human cancer. Cancer arises when cooperation among cells breaks down. The mutated cells revert to their primitive program of uncontrolled replication. We calculate the rate of activation of oncogenes and inactivation of tumor suppressor genes. We analyze the impact of mutations that trigger “genetic instability.” We outline the conditions necessary for “chromosomal instability” to initiate cancer progression. Chapter 13 is devoted to the evolutionary dynamics of the one trait that is truly our own invention and that is arguably the one interesting thing that has happened in the last six hundred million years on earth. Bacteria invented all the biochemistry of life. Eukaryotes invented some advanced genetics and how to build complicated multicellular plants and animals. Humans will be remembered for language. Chapter 14 summarizes and concludes. Further readings will be found at the back of the book.
IN 1831, at the age of twenty-two, Charles Darwin embarked on his journey around the world. He gazed at the breath-taking diversity of tropical flora and fauna, collected creepy-crawlies from the vast oceans that he traversed, was hopelessly seasick, saw slavery in Brazil, witnessed genocide in Argentina, and was underwhelmed by the naked humanity at Tierra del Fuego. He experienced the effects of a devastating earthquake in Chile that raised the South American continent. He led an expedition into the Andes and discovered marine fossils at high altitude. He paid little attention to which finches came from which islands in the Galápagos and ate most of the delicious turtles he had gathered on his way home across the Pacific. He saw Tahiti and the economic rise of Australia. He visited John Hershel, England’s leading physicist of the time, in South Africa; Hershel told him that “the mystery of mysteries” was the as yet unknown mechanism that gave rise to new species. Darwin returned to England’s shores after five years, having collected six thousand specimens that would require decades of analysis by an army of experts. His own observations in geology and the theory of his mentor, Sir Charles Lyell, that mountains were not lifted up in one day, but rose slowly over unimaginable periods of time, led Darwin to a key idea: given enough time everything can happen. Charles Darwin did not invent the concept of evolution. When he was a student in Edinburgh in the late 1820s, evolution was already the talk of the town. But evolution was rejected by the establishment. Those who adhered to evolutionary thinking were called Lamarckists, after the French scientist Jean-Baptiste Lamarck, who was the first to propose that species are not static, but change over time and give rise to new species. Lamarck had offered this perspective in a book published in 1809. He did not, however, propose a correct mechanism for how species change into each other. This mechanism was discovered first by Charles Darwin and independently by Alfred Russel Wallace. From reading the economist Thomas Malthus, Darwin was aware of the consequences of exponentially growing populations. Once resources become limiting only a fraction of individuals can survive. Darwin was also a keen observer of animal breeders. He analyzed their methods and studied their results. Slowly he understood that nature acted like a gigantic breeder. This was the first time that natural selection materialized as an idea, a scientific concept in a human mind. Darwin was thirty-three years old. The one problem that Darwin did not solve concerned the mechanism that could maintain enough diversity in a population for natural selection to operate. Darwin was unaware of the Austrian monk and botanist Gregor Mendel and his experiments on plant heredity. Mendel’s work had already been published but was hidden, gathering dust in the Annals of the Brno Academy of Sciences. Darwin once remarked, “I have deeply regretted that I did not proceed far enough at least to understand something of the great leading principles of mathematics; for men thus endowed seem to have an extra sense.” The engineer Fleeming Jenkins, who reviewed Darwin’s On the Origin of Species, published in 1859, had raised a fundamental and seemingly intractable objection to Darwin’s theory: if offspring inherit a blend of the parents’ characteristics, then variability diminishes in successive generations. Several decades later a simple mathematical equation, independently found by the famous British mathematician G. H. Hardy and the German physician Wilhelm Weinberg, showed that Mendelian (particulate) inheritance does lead to a maintainance of genetic diversity under random mating. The Hardy-Weinberg law is one of the fundamental principles of evolution under sexual reproduction. Mendelian genetics and Darwinian evolution were unified in the new discipline of mathematical biology, which developed from the seminal investigations of Ronald Fisher, J. B. S. Haldane, and Sewall Wright in the 1920s and 1930s. Through their work, fundamental concepts of evolution, selection, and mutation were embedded in a precise mathematical framework. This line of mathematical analysis was taken up in the 1950s by Motoo Kimura, who formulated the neutral theory of evolution. Kimura realized that most genetic mutations do not affect fitness and are fixed in populations only by random drift. Other milestones of evolutionary dynamics include William Hamilton’s discovery in 1964 that selection of “selfish genes” can favor altruistic behavior among relatives and John Maynard Smith’s invention of evolutionary game theory in 1973. In the mid-1970s Robert May revolutionized the mathematical approaches to ecology and epidemiology. Manfred Eigen and Peter Schuster formulated quasispecies theory, which provides a link between genetic evolution, physical chemistry, and information theory. Peter Taylor, Josef Hofbauer, and Karl Sigmund studied the replicator equation, the foundation of evolutionary game dynamics. This very brief and incomplete account of the evolution of evolutionary dynamics brings us to the present book. It has fourteen chapters. Although there is some progression of complexity, the chapters are largely independent. Therefore, if you know something about the subject, you can read the book in whatever order you like. My aim has been to keep things as simple as possible, as linear as possible, and as deterministic as possible. I will start with the basics and in a few steps lead you to some of the most interesting and unanswered research questions in the field. Having read the book, you will know what you need to embark on your own journey and make your own discoveries. This book represents an introduction to certain aspects of mathematical biology, but it is not comprehensive. Mathematical biology includes many topics, such as theoretical ecology, population genetics, epidemiology, theoretical immunology, protein folding, genetic regulatory networks, neural networks, genomic analysis, and pattern formation. The field is too diverse for any one book to represent it without running the risk of becoming as entertaining as a telephone directory. I have chosen those topics that I know well and where my explanation can be brief and effective. I have concentrated on evolution because it is the one unifying principle of all of biology. It might seem surprising that a book on evolutionary dynamics is not primarily about population genetics. Nevertheless the ideas and concepts of this fascinating field stand behind many of my explorations: the basic mathematical formulations of selection, mutation, random drift, fitness landscapes, and frequency-dependent selection as well as of evolution in structured populations have originated in population genetics. Several major themes of population genetics, however, such as sexual reproduction, sexual selection, recombination, and speciation, are not discussed here. In contrast, classical population genetics does not deal with evolutionary dynamics of infectious agents, the somatic evolution of cancer, evolutionary game theory, or the evolution of human language, all of which are subjects that I do explore. The main ingredients of evolutionary dynamics are reproduction, mutation, selection, random drift, and spatial movement. Always keep in mind that the population is the fundamental basis of any evolution. Individuals, genes, or ideas can change over time, but only populations evolve. The structure of the book is as follows. After this introduction, in Chapter 2 I will discuss populations of reproducing individuals and the basic ideas of natural selection and mutation. Simple models of population dynamics can lead to an exponential explosion, to a stable equilibrium, or to oscillations and chaos. Selection emerges whenever two or more individuals reproduce at different rates. Mutation means that one type can change into another. There are models of population growth that lead to the survival of whoever reproduces fastest (“survival of the fittest”). Other models lead to the survival of the first or the coexistence of all. In Chapter 3, quasispecies theory is introduced. Quasispecies are populations of reproducing genomes subject to mutation and selection. They live in sequence space and move over fitness landscapes. An important relationship between mutation rates and genome length is called the “error threshold”: adaptation on most fitness landscapes is possible only if the mutation rate per base is less than one over the genome length, measured in bases. In Chapter 4, we study evolutionary game dynamics, which arise whenever the fitness of an individual is not constant but depends on the relative abundance (= frequency) of others in the population. Thus evolutionary game theory is the most comprehensive way to look at the world. People who do not engage in evolutionary game theory restrict themselves to the rigidity of constant selection, where the fitness of one individual does not depend on others. The replicator equation is a nonlinear differential equation that describes frequency-dependent selection among a fixed number of strategies. We will encounter the Nash equilibrium and evolutionarily stable strategies. Evolutionary game theory and ecology are linked in an important way: the replicator equation is equivalent to the Lotka-Volterra equation of ecological systems, which describes the interation between predator and prey species. Chapter 5 is dedicated to the best game in town, the Prisoner’s Dilemma. The cooperation of reproducing entities is essential for evolutionary progress. Genes cooperate to form a genome. Cells cooperate to produce multicellular organisms. Individuals cooperate to form groups and societies. The emergence of human culture is a cooperative enterprise. The very problem of how to obtain cooperation by natural selection is described by the Prisoner’s Dilemma. In the absence of any other assumption, natural selection favors defectors over cooperators. Cooperation has a chance, however, if there are repeated interactions between the same two individuals. We will encounter the strategy Tit-for-tat, which is defeated first by Generous Tit-for-tat and then by Win-stay, lose-shift. In Chapter 6 we move to a stochastic description of finite populations. Neutral drift is a crucial aspect of evolutionary dynamics: if a finite population consists of two types of individuals, red and blue, and if both individuals have identical fitness, then eventually the population will be either all red or all blue. Even in the absence of selection, coexistence is not possible. If there is a fitness difference, then the fitter type has a greater chance of winning, but no certainty. We calculate the probability that the descendants of one individual will take over the whole population. This so-called fixation probability is important for estimating the rate of evolution. Chapter 7 is about games in finite populations. Most of evolutionary game theory has been formulated in terms of deterministic dynamics describing the limit of infinitely large populations. Here we move game theory to finite populations and make surprising observations. Neither a Nash equilibrium, nor an evolutionarily stable strategy, nor a risk-dominant strategy is protected by natural selection. There can be advantageous mutants against all three. In a bistable situation between two strategies, there is a simple “1/3 rule” that determines whether a strategy is favored by natural selection. In Chapter 8, the individuals of a population are represented by the vertices of a graph. The edges of the graph specify who interacts with whom. The graph can denote spatial relationships or social networks. The first observations of “evolutionary graph theory” are reported. The classical homogeneous population is defined by the complete graph, where all vertices are connected. We will see that circulations have the same evolutionary behavior as the complete graph in terms of fixation probability under constant selection, and therefore represent a particular balance between drift and selection. Graphs that enhance drift act as suppressors of selection. Graphs that reduce drift act as amplifiers of selection. In the limit of large population size, there exist graphs that guarantee the fixation of any advantageous mutant and the extinction of any disadvantageous mutant. Games on graphs are also studied in this chapter. There is a remarkably simple rule for the evolution of cooperation. Chapter 9 gives an account of evolutionary game dynamics on spatial grids. The primary approach will be deterministic, discrete in time, and discrete in space. This approach brings together game theory and cellular automata. We will observe evolutionary kaleidoscopes, dynamic fractals, and spatial chaos. There is all the complexity one could ever wish for—making it unnecessary for God to play dice. Moreover, cooperation can evolve on spatial grids. This is the concept of “spatial reciprocity.” In Chapter 10, we study the evolutionary dynamics of virus infections. I will argue that the mechanism of disease progression caused by the human immunodeficiency virus (HIV) is an evolutionary one. The immune system constantly attacks the virus, but the virus continuously evolves away, appears elsewhere in sequence space, and eventually overpowers the immune system. The resulting “diversity threshold theory” can explain why people succumb to the fatal immunodeficiency disease AIDS after a long and variable infection with HIV. Chapter 11 discusses the evolution of infectious agents, their attempts to infect new hosts, and the selection pressures that determine the level of virulence. The conventional wisdom is that well-adapted parasites are harmless to their hosts. This perspective is revised in the context of evolutionary dynamics. Competition between different mutants of a parasite maximizes its basic reproductive ratio. Superinfection takes into account that parasites compete on two levels of selection: within an infected host and in the population of hosts. Superinfection holds many surprising aspects, including the shortsighted evolution of higher and higher virulence beyond what would be optimum for the parasite. Chapter 12 explores the evolutionary dynamics of human cancer. Cancer arises when cooperation among cells breaks down. The mutated cells revert to their primitive program of uncontrolled replication. We calculate the rate of activation of oncogenes and inactivation of tumor suppressor genes. We analyze the impact of mutations that trigger “genetic instability.” We outline the conditions necessary for “chromosomal instability” to initiate cancer progression. Chapter 13 is devoted to the evolutionary dynamics of the one trait that is truly our own invention and that is arguably the one interesting thing that has happened in the last six hundred million years on earth. Bacteria invented all the biochemistry of life. Eukaryotes invented some advanced genetics and how to build complicated multicellular plants and animals. Humans will be remembered for language. Chapter 14 summarizes and concludes. Further readings will be found at the back of the book.

THIS CHAPTER introduces three basic building blocks of evolutionary dynamics: replication, selection, and mutation. These are the fundamental and defining principles of biological systems. They apply to any biological organization anywhere in our or other universes and do not depend on the particular details of which chemistry was recruited to embody life. Any living organism has arisen and is continually modified by these three principles. Evolution requires populations of reproducing individuals. In the right environment, biological entities, such as viruses, cells, and multicellular organisms can make copies of themselves. The blueprint that determines their structure, the genomic material in form of DNA or RNA, is replicated and passed on to the offspring. Selection results when different types of individuals compete with each other. One type may reproduce faster and thereby outcompete the others. Reproduction is not perfect, but involves occasional mistakes, or mutations. Mutation is responsible for generating different types that can be evaluated in the selection process, and thus results in biological novelty and diversity. Selection will choose to maintain some innovations and dismiss others, and can favor or oppose genetic diversity. At the end of this chapter we will focus on the Hardy-Weinberg law of random mating. This discussion will be our only venture into the mathematics of sexual reproduction. In subsequent chapters we will encounter additional principles of evolutionary dynamics, such as random drift and spatial movement. 2.1 REPRODUCTION Imagine a single bacterial cell in a perfect environment that contains all the nutrients required for growth and happiness. In this bacterial heaven, the fortunate cell and all its offspring divide every 20 minutes, which is the known world record for bacterial cell division in an ideal lab setting. After 20 minutes the cell has given rise to 2 daughter cells. After 40 minutes there are 4 granddaughters, and after one hour there are 8 great granddaughters. How many cells will there be after three days? After t generations there are 2t cells. In three days there are 216 generations. Hence we expect 2216 = 1065 cells. The total mass of these cells would exceed the mass of the earth by many orders of magnitude. The growth law for this overwhelming expansion can be written as a recursive equation Here xt is the number of cells at time t, and xt+1 is the number of cells at time t + 1. The equation means that at time t + 1 there are twice as many cells as at time t. Time is measured in numbers of generations. The number of cells at time 0 is given by x0. With this initial condition, the solution of equation (2.1) can be written as Equation (2.1) is a so-called difference equation, because time is measured in discrete steps. We can also formulate a differential equation for exponential growth that measures time as a continuous quantity. Let x(t) denote the abundance of cells at time t. Suppose that cells divide at rate r. More precisely, we assume that the time for cell division follows an exponential distribution with average 1/r. We can write the differential equation Throughout this book, I will use the standard notation to refer to differentiation (of x) with respect to time. If the abundance of cells at time 0 is given by x0 then the solution of the differential equation (2.3) is Let us reconsider our bacterial supernova. If we measure time in units of days, then r = 72 means that the time for a cell cycle requires, on average, 20 minutes (calculated by dividing the total number of minutes in a day, 1,440, by 72). Hence there are 72 cell divisions in one day. After three days, one bacterial cell has generated e216 cells which is approximately 6 × 1093 cells. The discrepancy between the differential equation and the difference equation is a consequence of the varying assumptions for the distribution of the generation time. The difference equation assumes that each cell division occurs after exactly 20 minutes. The differential equation assumes that each cell division occurs after a time which is exponentially distributed around an average of 20 minutes. The exponential distribution is defined as follows: the probability that cell division occurs between time 0 and τ is given by 1 − e−rτ. On average, cells divide after 1/r time units. So far we have ignored cell death. Let us now suppose that cells die at rate d, which means that they have an exponentially distributed lifespan with an average of 1/d. The differential equation becomes The effective growth rate is the difference between the birth rate, r, and the death rate, d. If r > d, then the population will expand indefinitely. If r < d, then the population will converge to zero and become extinct. If r = d, then the population size remains constant, but this situation is unstable: small deviations from absolute equality between birth and death will lead to either exponential expansion or decline. It is important to note that setting r = d in equation (2.5) does not constitute a mechanism for maintaining a stable constant population size. The simple equation (2.5) allows us to introduce an extremely important concept in evolution, ecology and epidemiology: the basic reproductive ratio, r/d. This ratio denotes the expected number of offspring that come from any one individual. The average lifetime of a cell is 1/d. The rate of producing offspring cells is given by r. If each cell produces on average more than one offspring, r/d > 1, then an exponential expansion will follow. A basic reproductive ratio greater than one is a necessary condition for population expansion. We have observed that ongoing exponential growth can lead to unreasonably high numbers in a very short time. In a realistic environment, the expanding population will hit constraints that prevent further expansion. For example, the population might run out of nutrients or physical space. A model for population expansion with a maximum carrying capacity is given by the logistic equation As before, the parameter r refers to the rate of reproduction in the absence of density regulation, when the population size, x, is much smaller than the carrying capacity K. As x increases, the rate of growth slows down. When x reaches the carrying capacity, K, then the population expansion ceases. For the initial condition x0, the solution of equation (2.6) is given by In the limit of infinite time, t → ∞, the population size converges to the equilibrium x* = K. Throughout the book we will use a superscript asterisk to denote a quantity at equilibrium. 2.1.1 Deterministic Chaos We can also study a logistic difference equation. Without loss of generality, let us rescale the population abundance in such a way that the maximum carrying capacity is given by K = 1. We have Note that the growth rate in the difference equation, a, is analogous to 1 + r in the differential equation (2.6). In contrast to the differential equation, the logistic difference equation (2.8) has many surprises. The behavior of this equation is so rich that many papers and even books have been written about it, and it has generously awarded glorious careers to some scientists who have studied it. The abundance of the population, x, is given by a number between 0 and 1. Thegrowthrate, a, can vary between 0 and 4. If a < 0 or a > 4, then negative x values will be generated, which are not biologically meaningful. The point x = 0 is always an equilibrium. If a < 1, then the only stable equilibrium of the system is given by x* = 0. This means the population will die out. If 1 < a < 3, then the only stable equilibrium is given by x* = (a − 1)/a. All trajectories starting from any initial condition x0 (greater than 0 and less than 1) will converge to this value. The point x* is a global attractor for the open interval (0, 1). If a > 3, then the point x* becomes unstable. For a values slightly above 3, we find a stable oscillation of period two. As a increases, the period two oscillator is replaced by period four, then by eight, and so on. For a = 3.57 there are infinitely many even periods. For a = 3.6786 the first odd periods appear. For 3.82 < a ≤ 4 all periods occur. The logistic map with a = 4 is a simple and most illuminating example for studying deterministic chaos. For any value xt it is straightforward to compute the population size in the subsequent generation, xt+1. Yet the dynamics are unpredictable in the following sense. Suppose the value of xt is only known subject to a small uncertainty. It may not be clear whether xt = 0.3156 or 0.3157. After ten generations, however, the trajectories starting from these two initial values will have diverged completely. Hence prediction is impossible. Anything can happen.
We conclude that simple rules can generate complicated behavior. Much of the apparent complexity and unpredictability of biological time series, such as the population size of birds in a particular habitat, the number of measles cases in New York City, or the price fluctuations of stocks and bonds, could in principle be the consequence of deterministic laws. Figure 2.1 Evolution requires a population of reproducing individuals. Strictly speaking, neither genes, nor cells, nor organisms, nor ideas evolve. Only populations can evolve. 2.2 SELECTION Selection operates whenever different types of individuals reproduce at different rates. At the very least we need two types (Figure 2.1). Let us call them A and B. Type A individuals reproduce at rate a. Type B individuals reproduce at rate b. The rate of reproduction is interpreted as fitness. Therefore the fitness of A is a, the fitness of B is b. Denote by x(t) the number of A individuals at time t. Denoteby y(t) the number of B individuals at time t. At time t = 0, the numbers of A and B are respectively given by x0 and y0. The A and B subpopulations grow according to the differential equations Equation (2.9) is a system of two ordinary, linear differential equations. The analytical solution is given by Hence the A and B subpopulations grow exponentially at rates a and b, respectively. The doubling time for A is log 2/a. The doubling time for B is log 2/b. If a is greater than b, then A reproduces faster than B: after some time, there will be more A than B individuals. Denote by ρ(t) = x(t)/y(t) the ratio of A over B at time t. We have The solution of this differential equation, for the initial condition ρ0 = x0/y0, is given by Hence if a > b then ρ tends to infinity. In this case A will outcompete B, which means selection favors A over B. If, on the other hand, a < b, then ρ tends to zero. In this case B will outcompete A, which means that selection favors B over A. Let us now consider a situation in which the total population size is held constant. This situation can arise, for example, when an ecosystem has a constant maximum carrying capacity. Let x(t) denote the relative abundance of A at time t. Instead of “relative abundance” we can also say “frequency.” Let y(t) denote the frequency of B. Since there are only A and B individuals in the population, we have x + y = 1. As before, A and B individuals reproduce, respectively, at rates a and b. We have the system of equations The term ϕ ensures that x + y = 1. This is only possible if ϕ = ax + by. Observe that ϕ is the average fitness of the population. The system (2.13) describes only a single differential equation, because y can be replaced by 1 − x. We obtain
Figure 2.2 Selection arises if two types, A and B, have different rates of reproduction, a and b. If A reproduces faster than B, which means a > b, then A will become more abundant than B. Eventually A will take over the entire population; B will become extinct. Denote by x the relative abundance (= frequency) of type A. The quantity x is a number between 0 and 1. Therefore selection dynamics are defined on the closed interval [0, 1]. This differential equation has two equilibria, one for x = 0 and the other for x = 1. At these two points, we have = 0. This observation makes sense: if x = 1 then the system consists only of A individuals and nothing more can happen; if x = 0, then the system consists only of B individuals and again nothing more can happen. We can, however, make an additional observation. If a >b, then > 0 for all values of x that are strictly greater than 0 and strictly smaller than 1. This means that for any mixed system (consisting of some A and some B individuals) the fraction of A will increase if the fitness of A is greater than the fitness of B. In this case, the fraction of B will converge to 0, while the fraction of A converges to 1. We have encountered the concept of “survival of the fitter” (Figure 2.2). 2.2.1 Survival of the Fittest The model can be extended to describe selection among n different types. Let us label them i = 1,…, n. Denote by xi(t) the frequency of type i. The structure of the population is given by the vector Denote by fi the fitness of type i. As before, fitness is a non-negative real number and describes the rate of reproduction. The average fitness of the population is given by Figure 2.3 If the total population size is constant, then selection dynamics can be formulated in terms of relative abundance (= frequency). Suppose there are n different types, i = 1,…, n. Type i has frequency xi. The sum over all xi is one. The set of all points, (x1,…, xn) with the property , is called the simplex Sn. Selection dynamics occur on the simplex Sn. The figure shows S2, S3, and S4. The simplex Sn is an n − 1 dimensional structure embedded in an n-dimensional Euclidian space. The simplex Sn has n faces that each consist of the simplex Sn−1. Selection dynamics can be written as The frequency of type i increases, if its fitness exceeds the average fitness of the population. Otherwise it will decline. The total population size remains constant: and . The set of points with the property is called the simplex Sn (Figure 2.3). Each point in the simplex refers to a particular structure of the population. The interior of the simplex is the set of points with the property that xi > 0 for all i = 1,…, n. The face of the simplex is the set of points with the property that xi = 0 for at least one i. The vertices of the simplex are the corner points where exactly one type is present, xi = 1, while all other types are extinct, xj = 0 for all j ≠ i (Figures 2.4 and 2.5). Figure 2.4 The interior of a simplex is the set of all points where all coordinates are strictly positive; this means no type has become extinct. The faces are the sets of points where at least one coordinate is zero; this means at least one type has become extinct. The vertices describe pure populations, where all but one type have become extinct. The simplex S2 is given by the closed interval [0, 1]. The notation [0, 1] refers to all numbers which are greater than or equal to 0 and less than or equal to 1. In contrast, (0, 1) is the open interval; it contains all numbers that are strictly greater than 0 and strictly less than 1. The open interval (0, 1) is the interior of the closed interval [0, 1] and, therefore, is also the interior of the simplex S2. Equation (2.16) contains a single globally stable equilibrium. Starting from any initial condition in the interior of the simplex, the population will converge to a corner point where all but one type have become extinct. The winner, k, enjoys a well-deserved victory because it has the property of having the largest fitness, fk. Thus fk > fi for all i = k. The system shows competitive exclusion: the fittest type will outcompete all others. This is the concept of “survival of the fittest.” 2.2.2 Survival of the First, Survival of All Let us return to the selection of two types, A and B, but without making the assumption that their growth rates are linear functions of their frequencies. Instead consider the equation
Figure 2.5 Five points on the simplex S3. In the center, (1/3, 1/3, 1/3), all three types have the same frequency. There are three faces. The center of one particular face is given by (0, 1/2, 1/2); one type has become extinct. The corner points (vertices) indicate populations that consist of only one type. S3 has three corners: (1, 0, 0),(0, 1, 0), and (0, 0, 1).
Figure 2.5 Five points on the simplex S3. In the center, (1/3, 1/3, 1/3), all three types have the same frequency. There are three faces. The center of one particular face is given by (0, 1/2, 1/2); one type has become extinct. The corner points (vertices) indicate populations that consist of only one type. S3 has three corners: (1, 0, 0),(0, 1, 0), and (0, 0, 1). As before, a and b denote the fitness values of A and B, respectively. If c = 1, we are back to equation (2.13). If c < 1, then growth is subexponential. In the absence of the density limitation, ϕ, the growth curve of the two types would be slower than exponential. In contrast, if c > 1, then growth is superexponential. In the absence of the density limitation, ϕ, the growth curve of the two types would be faster than exponential (hyperbolic). To maintain a constant population size, x + y = 1, we set ϕ = axc + byc. Equation (2.17) reduces to where Figure 2.6 Survival of all: for subexponential growth (c < 1), there is a stable mixed equilibrium between A and B, even if one type has a faster growth rate than the other. Survival of the first: for superexponential growth (c > 1), there is an unstable mixed equilibrium between A and B, while the pure populations are stable. For example, if the whole population consists of B, then A cannot invade even if it has a higher growth rate. This equation always has fixed points for x = 0 and x = 1. For c ≠ 1 there exists exactly one other fixed point between 0 and 1. It is given by If c < 1, then the boundary fixed points, x = 0 and x = 1, are always unstable; the interior fixed point, x*, is globally stable. Hence there is survival of both A and B. Surprisingly, even if A is fitter than B, a > b, then a small amount of B can invade an A population. If c > 1, then the boundary fixed points, x = 0 and x = 1, are always stable; the interior fixed point, x*, is unstable. If x > x*, then A will outcompete B. If x < x*, then B will outcompete A. Again this observation is remarkable. Even if A is fitter than B in the sense that a > b, a B population cannot be invaded by an A mutant. We conclude that superexponential growth favors whoever was there first (survival of the first) whereas subexponential growth leads to the survival of all (Figure 2.6). What is the intuition behind this observation? An extreme form of subexponential growth is “immigration,”c = 0. The growth rate does not depend on x or y at all. We have with ϕ = a + b. This equation can be interpreted as the immigration of A and B into the population from some other place. It is clear that these immigration dynamics lead to coexistence. A value of c between 0 and 1 is a mixture between immigration and linear growth and retains the property of coexistence. If c > 1, on the other hand, then A cannot invade B even if a > b. (“Invasion” means that an infinitesimally small fraction of A individuals can increase in abundance in a population where almost all individuals are of type B.) The intuitive explanation is as follows: we can think of the case c = 2 as implying that two individuals of the same type have to meet in order to reproduce. If there is only an infinitesimally small fraction of A individuals, then two A individuals will
never meet and hence A will not reproduce. If c = 3 then three individuals of the same type have to meet in order to reproduce. Again arbitrarily small fractions of a type can never increase. The same intuition holds for all values c > 1. The case c = 2 can also be interpreted as an evolutionary game between two strategies, A and B, that are strict Nash equilibria. Neither strategy can invade the other. We will encounter these concepts in Chapter 4. 2.3 MUTATION Life takes advantage of mistakes. Replication of DNA or RNA can lead to slightly modified sequences that represent novel variants. Errors during reproduction are called mutations. In this section, we study the simplest possible differential equations that describe mutation (Figure 2.7). Let us again consider just two types, A and B. Denote by u1 the mutation rate from A to B: u1 is the probability that the reproduction of A leads to B. Figure 2.7 Mutation can occur during reproduction: type A produces an offspring that is type B. Mutation can also occur in the absence of reproduction: type A changes into type B. Many genetic mutations occur when the genomic material of a cell is being copied. But mutagens can also change the genetic material of a cell when it is not dividing. Conversely, denote by u2 the mutation rate from B to A. As before, let x and y denote the frequencies of A and B, respectively. We have Since A and B have the same fitness (a = b = 1), the average fitness of the population is constant and given by ϕ = 1. Taking into account x + y = 1, system (2.22) reduces to the differential equation The frequency of A converges to the stable equilibrium Hence mutation leads to coexistence between A and B. The relative proportion of A and B at equilibrium depends on the mutation rates. At equilibrium, the ratio of A to B is given by x*/y* =u2/u1. If the mutation rates are the same, u1 = u2, and then x* = y*. Sometimes the mutation rate in one direction is much larger than in the other direction. In these cases, it often makes sense to ignore mutation in the other direction altogether. Let u2 = 0. We have Therefore the frequency of A declines over time as The frequency of B increases as If mutation occurs only from A to B but not the other way around, then A will die out and B will take over the whole population. We see that mutation can affect survival. Different mutation rates can introduce selection even in the absence of different reproductive rates. 2.3.1 Mutation Matrix We can extend mutation dynamics to n different types. Let us introduce the mutation matrix, Q = [qij]. The probability that type i mutates to type j is given by qij. Since each type i has to produce itself or some other type, we have Thus Q is a stochastic n × n matrix. A stochastic matrix is defined by the properties that (i) all entries are numbers from the interval [0, 1] (so-called probabilities), (ii) there are as many rows as columns, and (iii) the sum of each row is 1. Stochastic matrices always have 1 as an eigenvalue, and no eigenvalue has an absolute value greater than 1. Mutation dynamics can be written as In vector notation we can write Again the average fitness is just ϕ = 1. The equilibrium is given by the left-hand eigenvector associated with eigenvalue 1: The point denotes the unique globally stable equilibrium of the mutation dynamics. 2.4 MATING One of the problems that Charles Darwin could not solve was the following: under random mating and blending inheritance, the variability in a population should rapidly decline. Yet it was clear that variability was needed for natural selection. If variability disappears, then natural selection has nothing upon which to act. Suppose there is a distribution of body size in a population. If children inherit the average body size of their parents, then after some time everybody is the same size. Under these circumstances, how can natural selection affect changes in body size? The first part of the solution is that inheritance (on the level of genes) is not blending but particulate, as had been discovered by Gregor Mendel and published in 1866. That is, individuals have discrete genotypes that get reshuffled, not blended, during mating. Mendel’s work was unknown to Darwin. The second step was a simple mathematical analysis, which was performed by the British mathematician G. H. Hardy, who was proud never to have done anything useful (= applied) in his life, only to have his name forever associated with a highly useful and very applied concept in population genetics. Moreover, Hardy’s brief calculation was generalized by the German physician Wilhelm Weinberg. Consider an infinitely large population of a diploid organism with two sexes and random mating (a diploid organism has two copies of its genome; humans and many other animals are diploid). Let us look at one particular gene locus and assume there are two alleles, A1 and A2. The alleles are variants of the same gene and might differ in one or a few point mutations. (Point mutation means that only one single base of the DNA sequence is changed.) There are 3 different genotypes: A1A1, A1A2, A2A2. Let us denote their frequencies in the population by x, y, and z, respectively. Denote by p and q the frequencies of alleles A1 and A2. We have x + y + z = 1 and p + q = 1. Moreover,
Let us now assume random mating. In the next generation, the genotype frequencies are given by For the allele frequencies in the next generation we have again Combining (2.32) and (2.33), we observe that Therefore the allele frequencies remain unchanged from one generation to the next. Moreover, combining (2.32) and (2.34), we observe From the first generation on, the genotype frequencies can be directly derived from the allele frequencies. Note that equation (2.35) need not hold for the initial genotype and allele frequencies. The Hardy-Weinberg law (expressed by equations 2.34 and 2.35) can be generalized to n alleles. In summary, the Hardy-Weinberg law states that particulate inheritence preserves variation within a population under random mating. SUMMARY Evolution requires populations of reproducing individuals. Asexual reproduction leads to exponential population growth (which will eventually be checked by resource limitation). Simple models of population growth in discrete time can give rise to very complicated dynamics. Selection arises when different types of individuals reproduce at different rates. Normally, the faster-reproducing (fitter) individual outcompetes the slower reproducing (less fit) individual. If there are many different types, then selection dynamics can lead to “survival of the fittest.” All others become extinct. Sublinear growth rates lead to coexistence, “survival of all.” Superlinear growth rates prevent invasion of a new type and thereby lead to “survival of the first.” Mutation arises when reproduction is not perfectly accurate. Mutation promotes coexistence of different types. Asymmetric mutation can lead to selection even if all individuals have the same reproduction rate. The Hardy-Weinberg law states that random mating preserves genetic variation within a population.

GENOMES ARE SEQUENCES of the four-letter alphabet A, T, C, G, denoting the nucleotides adenine, thymine, cytosine, and guanine. All living cells use double-stranded DNA to carry their genomic information. Many viruses also use DNA, but some viruses encode their genome in form of RNA. The genome length of organisms varies greatly, ranging from about 104 nucleotides for small viruses, to 106 for bacteria to 3 × 109 for humans. Curiously, newts and lungfish “need” an even larger genome than do humans (19 × 109 and 140 × 109, respectively). The evolutionary dynamics of genome size and genome organization is a fascinating topic. If a cell wants to produce a particular protein, then the DNA of the corresponding gene is “transcribed” into messenger RNA (mRNA), which is in turn “translated” into protein. The transcription is done by particular enzymes called DNA-dependent RNA polymerases. The translation is performed by a complicated arrangement of RNA and proteins called ribosomes. The words “transcription” and “translation” were invented by the mathematician John von Neumann when he calculated how to build a self-reproducing machine. He came up with an architecture equivalent to the organization of cells some decades before molecular biology had been invented. RNA also uses a four-letter alphabet, A, U, C, G. Thymine is replaced by uracil. Furthermore, the sugar backbone of RNA has an additional -OH (hydroxy) group, which makes the molecule less stable and more dynamic. DNA is a stable carrier of information. RNA also carries information, but in addition some RNAs have enzymatic activity. Proteins consist of 20 amino acids. Each amino acid is encoded by a sequence of three letters of the RNA alphabet. This genetic code is essentially the same for all living cells, ranging from bacteria to humans to newts. Hence the genetic code is believed to have originated only once: in the first cell that is ancestor to all existing cells. A 4-letter alphabet generates 64 possible sequences of length 3. Since there are only 20 amino acids, the genetic code is redundant: some amino acids are encoded by more than one sequence. Some sequences are used to signal the end of the transcription process. We see that molecular biology adds a precise information-theoretic perspective to evolutionary dynamics. 3.1 SEQUENCE SPACE In the green hills of Sussex lived an imaginative theoretical biologist, John Maynard Smith, who once pictured all proteins (of a certain length) arranged in such a way that nearest neighbors differed by a single amino acid. This was the origin of what we call “sequence space” as a concept in the human mind. Let us consider all proteins of the modest length 100. Each position of the protein sequence is filled by one of 20 amino acids. Hence, this space has 100 dimensions and in total 20100 points. This number corresponds to 10130 proteins. In contrast, there are only some 1080 estimated protons in our universe. Nor is there any reason for us to consider only proteins of length 100; some proteins are much longer. We conclude there are many more possible proteins than available protons and hence evolution so far has and, for the remaining 1030 years that constitute the lifetime of our protons, will only explore a vanishingly small subset of all possible proteins. What is true of proteins is also true of genes and genomes. We can imagine all nucleotide sequences of a certain length arranged in a way that nearest neighbors differ in one position. For sequence length L this generates a lattice in an L-dimensional space. In each dimension there are 4 discrete possibilities. Hence there are 4L possible sequences. Figure 3.1 Genomes live in sequence space. The number of dimensions is given by the length of the genome. Small viruses live in 10,000 dimensions. Humans live in about 3 billion dimensions. For writing computer programs, it is often convenient to use binary sequences, the fundamental strings of silicon thoughts. Moreover, everything from Shakespeare to E. coli can be encoded in binary sequences. For length L there are 2L possibilities. In Figure 3.1, the binary sequence space for L = 3 is shown. The distance between 000 and 010 is one. The distance between 000 and 011 is 2 (and not ). Hence sequence space is characterized not by a Euclidean metric but by a so-called Hamming metric or Manhattan metric. In Manhattan, if you are on 5th Avenue and 51st Street it takes 2 blocks to go to 6th Avenue and 52nd Street, not blocks. This metric was introduced by Richard Hamming in information theory. Let us compare the binary sequence space of length L = 300 with a three-dimensional cubic lattice containing the same number of points. There are 2300 ≈ 1090 points. Imagine nearest neighbors are placed at a distance of 1 meter. The diagonal of the three dimensional cubic lattice has a length of about 1030 meters, which corresponds to about 1014 light years. In contrast, the longest distance in the L-dimensional hypercube is only 300 meters. Thus sequence space is characterized by short distances, but many dimensions. It is not far to move from one sequence to another, but there are many possible steps that lead in wrong directions. Evolution is a trajectory through sequence space. This trajectory needs an efficient guide.
Figure 3.2 The fitness landscape is a high-dimensional mountain range. Each genome (= each point in sequence space) gets assigned a fitness value. 3.2 FITNESS LANDSCAPES The American population geneticist Sewall Wright invented the concept of a “fitness landscape” in the 1930s, but Manfred Eigen and Peter Schuster, collaborating in the 1970s, combined fitness landscape with sequence space. Consider a function that assigns to each genomic sequence a fitness value. Hence we build a mountain range on the foundation of an L-dimensional sequence space (Figure 3.2). This mountain range has L + 1 dimensions. The evolutionary process of mutation and selection explores this hyper-alpine mountain range. The genomic sequence represents the genotype of an organism. The phenotype of an organism is given by its shape, behavior, performance and any kind of ecological interaction. The phenotype determines the fitness (reproductive rate) of the organism. There is a mapping from genotype to phenotype. There is another mapping from phenotype to fitness. The fitness landscape is a convolution of these two mappings. It is a direct mapping from genotype to fitness. Figure 3.3 The ensemble of genomes of a natural population form a quasispecies: the genomes of different individuals are similar but not identical. Biology has chosen a four-letter alphabet consisting of the nucleotides A, T, C, and G for its genes. Most in silico evolution uses a binary alphabet for convenience. Sequence differences (mutations) are shown in red. The fitness landscape of certain problems can be determined experimentally. For example, HIV can generate point mutations that confer drug resistance. The relative growth rate of such mutants can be determined by in-vitro assays. In general, however, to understand the relationship between genotype, phenotype, and fitness is an extremely complicated problem. Much of biology, including developmental biology, molecular biology, post-genomics, and proteomics, is devoted to this very task. 3.3 THE QUASISPECIES EQUATION A quasispecies is an ensemble of similar genomic sequences generated by a mutation-selection process (Figure 3.3). The term was introduced by the chemists Manfred Eigen and Peter Schuster. In chemistry the word “species” refers to an ensemble of identical molecules, for example, the species of all water molecules. But the species of all RNA molecules does not contain identical sequences, and therefore the term “quasispecies” was coined. Biologists are sometimes confused by this expression, because they relate it to the concept of a biological species. We stay with binary sequences for convenience. We note that any genomic or other information can be encoded by binary sequences. Consider all binary sequences of length L. Enumerate all those sequences by i = 0, 1, 2,…, n where n = 2L − 1. A natural enumeration is obtained if the sequence represents the binary description of the corresponding integer. For example, let L = 4. The sequence 0000 corresponds to i = 0, the sequence 0001 to i = 1, the sequence 0010 to i = 2,…, the sequence 1111 to i = 15. Imagine an infinitely large population of organisms, each carrying a genome of length L. Denote by xi the relative abundance (= frequency) of those organisms that contain genome i. We have The genomic structure of the population is given by the vector Denote by fi the fitness of genome i. It is a non-negative real number. Thus genomes of type i are being reproduced at rate fi. The fitness landscape is given by the vector The average fitness of the population, is the inner product of the vectors and . We have During replication of a genome, mistakes can happen. The probability that replication of genome i results in genome j is given by qij. Here we again meet the mutation matrix Q = [qij] of section 2.3. We remember that Q is a stochastic matrix: it has as many rows as columns; each entry is a probability, which means a number between 0 and 1; each row sums to one, The quasispecies equation (Figure 3.4) is given by Sequence i is obtained by replicating any sequence j at rate fj times the probability that replication of sequence j generates sequence i. Each sequence is removed at rate ϕ to ensure that the total population size remains constant, Thus quasispecies dynamics are defined on the simplex, Sn. In the limiting case of completely error-free replication, Q becomes the identity matrix: all diagonal entries are one, all off-diagonal entries are zero. Consider an initial condition in the interior of the simplex, defined by xi > 0 for all i. The quasispecies will converge to a homogeneous population that consists only of the fittest sequence. If f0 > fi for all i ≠ 0, then the stable equilibrium is given by x0 = 1 and xi = 0 for i ≠ 0. If there are no errors, then the quasispecies equation (3.1) reduces to the selection equation (2.16) of section 2.2.1. Figure 3.4 The quasispecies equation, formulated by Manfred Eigen and Peter Schuster, is one of the most important equations in theoretical biology. It describes the mutation and selection of an infinitely large population on a constant fitness landscape. Let us now assume that errors occur. This means that (at least some) off-diagonal entries of Q are not zero. In many realistic contexts, the matrix Q is irreducible, which means it is possible to find a sequence of mutations from any one genome i to any other genome j. Furthermore, let fi > 0 for at least some i. In this case, the quasispecies equation admits a single, globally stable equilibrium, , in the simplex Sn. The equilibrium quasispecies, , does not necessarily maximize the average fitness ϕ. Consider again a fitness landscape with the property f0 > fi for all i ≠ 0. Then the population consisting only of sequence 0 will have a higher fitness than the equilibrium population . Thus, mutations reduce the average fitness at equilibrium.
